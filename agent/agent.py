# agent/agent.py
from agent.memory import memory
from agent.router import route_message
from agent.prompts import SYSTEM_PROMPT
from agent.models import send_to_llm
from vector_store import vector_store
import logging

logger = logging.getLogger(__name__)

DOC_INDEX_USER_ID = "shared"  # –û–±—â–∏–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

async def agent_process(prompt: str, user_id: str):
    history = memory.get_history(user_id) or []

    rag_context = _get_rag_context(prompt)

    enhanced_system_prompt = SYSTEM_PROMPT
    if rag_context:
        enhanced_system_prompt += f"\n\nüìö **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:**\n{rag_context}"

    messages = [{"role": "system", "content": enhanced_system_prompt}] + history
    messages.append({"role": "user", "content": prompt})

    if vector_store.is_connected():
        vector_store.add_chat_message(prompt, "user", user_id)

    result, updated_messages = await route_message(messages, user_id)

    if result is None:
        logger.info("–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ LLM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")
        result = await send_to_llm(updated_messages)
        updated_messages.append({"role": "assistant", "content": result})

    if vector_store.is_connected():
        vector_store.add_chat_message(result, "assistant", user_id)

    memory.set_history(user_id, updated_messages[-50:])
    return result


def _get_rag_context(query: str, max_length: int = 2000) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –æ–±—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–∞–º—è—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    if not vector_store.is_connected():
        return ""

    try:
        context_parts = []

        # –ü–æ–∏—Å–∫ –ø–æ –æ–±—â–µ–º—É –∏–Ω–¥–µ–∫—Å—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_results = vector_store.search_documents(query, DOC_INDEX_USER_ID, limit=3)
        if doc_results:
            context_parts.append("**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:**")
            for doc in doc_results:
                content_preview = doc["content"][:300]
                if len(doc["content"]) > 300:
                    content_preview += "..."
                context_parts.append(f"‚Ä¢ [{doc['filename']}]: {content_preview}")

        # –ü–æ–∏—Å–∫ –ø–æ –ø–∞–º—è—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_facts = vector_store.search_memory(query, DOC_INDEX_USER_ID, limit=2)
        if user_facts:
            context_parts.append("\n**–ß—Ç–æ —è –∑–Ω–∞—é –æ –≤–∞—Å:**")
            for fact in user_facts:
                context_parts.append(f"‚Ä¢ {fact}")

        # –ü—Ä–æ—à–ª—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        chat_history = vector_store.search_chat_history(query, DOC_INDEX_USER_ID, limit=2)
        if chat_history:
            context_parts.append("\n**–ò–∑ –ø—Ä–æ—à–ª—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤:**")
            for chat in chat_history:
                message_preview = chat["message"][:200]
                if len(chat["message"]) > 200:
                    message_preview += "..."
                context_parts.append(f"‚Ä¢ {message_preview}")

        full_context = "\n".join(context_parts)
        if len(full_context) > max_length:
            full_context = full_context[:max_length] + "..."
        return full_context

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
        return ""