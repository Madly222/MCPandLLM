import re
import json
import logging

from agent.memory import memory
from agent.router import route_message
from agent.prompts import SYSTEM_PROMPT
from agent.models import send_to_llm
from vector_store import vector_store
from tools.search_tool import get_rag_context
from tools.edit_excel_tool import edit_excel
from tools.file_generator_tool import parse_llm_json, build_from_json

logger = logging.getLogger(__name__)

MAX_HISTORY = 10
MAX_CONTEXT_CHARS = 60000


def _is_simple_message(query: str) -> bool:
    query_lower = query.lower().strip()

    simple = [
        r'^(Ð¿Ñ€Ð¸Ð²ÐµÑ‚|Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹|Ð´Ð¾Ð±Ñ€Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ|Ð´Ð¾Ð±Ñ€Ñ‹Ð¹ Ð²ÐµÑ‡ÐµÑ€|Ð´Ð¾Ð±Ñ€Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾|Ñ…Ð°Ð¹|hello|hi|hey)[\s!.?]*$',
        r'^(Ð¿Ð¾ÐºÐ°|Ð´Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ|bye|goodbye)[\s!.?]*$',
        r'^(ÑÐ¿Ð°ÑÐ¸Ð±Ð¾|Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€ÑŽ|thanks|thank you)[\s!.?]*$',
        r'^(Ð´Ð°|Ð½ÐµÑ‚|Ð¾Ðº|okay|ok|Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾|Ð¿Ð¾Ð½ÑÐ»|ÑÑÐ½Ð¾|ÑƒÐ³Ñƒ)[\s!.?]*$',
        r'^(ÐºÐ°Ðº Ð´ÐµÐ»Ð°|ÐºÐ°Ðº Ñ‚Ñ‹|Ñ‡Ñ‚Ð¾ Ð½Ð¾Ð²Ð¾Ð³Ð¾)[\s?]*$',
        r'^(ÐºÑ‚Ð¾ Ñ‚Ñ‹|Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹|Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ|help)[\s?]*$',
    ]

    for p in simple:
        if re.match(p, query_lower):
            return True
    return False


def _extract_and_apply_operations(llm_response: str, role: str = None) -> str:
    logger.info(f"Checking LLM response for JSON ({len(llm_response)} chars)")

    json_data = parse_llm_json(llm_response)

    if not json_data:
        logger.info("No valid JSON found in response")
        return llm_response

    if "sheets" in json_data:
        logger.info("Found file generation JSON")

        state = memory.get_state(role) or {}
        pending = state.get("pending_template_build", {})

        result = build_from_json(
            json_data,
            template_name=pending.get("template"),
            role=role
        )

        state["pending_template_build"] = None
        memory.set_state(role, state)

        if result.get("success"):
            explanation = _extract_explanation(llm_response)
            response = f"âœ… Ð¤Ð°Ð¹Ð» ÑÐ¾Ð·Ð´Ð°Ð½: {result['filename']}\n"
            response += f"ðŸ“Š Ð›Ð¸ÑÑ‚Ð¾Ð²: {result.get('sheets_count', 0)}, "
            response += f"Ð¡Ñ‚Ñ€Ð¾Ðº: {result.get('rows_count', 0)}\n"
            response += f"ðŸ”— Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ: {result['download_url']}"
            if explanation:
                response = f"{explanation}\n\n{response}"
            return response
        else:
            return f"{llm_response}\n\nâŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð°: {result.get('error')}"

    if "operations" in json_data:
        logger.info("Found edit operations JSON")

        filename = json_data.get("filename")
        operations = json_data.get("operations", [])

        if not filename or not operations:
            logger.warning("Missing filename or operations")
            return llm_response

        logger.info(f"Applying {len(operations)} operations to: {filename}")

        result = edit_excel(filename, operations, role=role)

        if result.get("success"):
            explanation = _extract_explanation(llm_response)
            if explanation:
                return f"{explanation}\n\nÐ“Ð¾Ñ‚Ð¾Ð²Ð¾! Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ: {result['download_url']}"
            else:
                return f"Ð¤Ð°Ð¹Ð» Ð¾Ñ‚Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½!\n\nÐ¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ: {result['download_url']}"
        else:
            return f"{llm_response}\n\nÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ: {result.get('error')}"

    return llm_response


def _extract_explanation(response: str) -> str:
    json_start = response.find('```json')
    if json_start == -1:
        json_start = response.find('{')

    if json_start > 0:
        explanation = response[:json_start].strip()
        if explanation:
            return explanation

    return ""


async def agent_process(prompt: str, role: str):
    history = (memory.get_history(role) or [])[-MAX_HISTORY:]

    rag_context = ""
    if not _is_simple_message(prompt):
        logger.info(f"Running RAG search for: {prompt[:50]}...")
        rag_context = get_rag_context(prompt, role, top_n=10, max_context_chars=MAX_CONTEXT_CHARS)
        if rag_context:
            logger.info(f"RAG context: {len(rag_context)} chars")

    system_content = SYSTEM_PROMPT
    if rag_context:
        system_content += f"\n\n{rag_context}"

    messages = [{"role": "system", "content": system_content}] + history
    messages.append({"role": "user", "content": prompt})

    total_chars = sum(len(m["content"]) for m in messages)
    logger.info(f"Prompt: {total_chars} chars, ~{total_chars // 4} tokens")

    if vector_store.is_connected():
        vector_store.add_chat_message(prompt, "user", role)

    result, updated_messages = await route_message(messages, role)

    if result is None:
        llm_response = await send_to_llm(updated_messages)
        result = _extract_and_apply_operations(llm_response, role)

    if vector_store.is_connected():
        vector_store.add_chat_message(result, "assistant", role)

    new_history = history + [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": result}
    ]
    memory.set_history(role, new_history[-MAX_HISTORY:])

    return result