import logging
from pathlib import Path
from typing import List, Dict, Optional

from vector_store import vector_store
from tools.utils import BASE_FILES_DIR
from tools.file_tool import read_file
from tools.excel_tool import read_excel

logger = logging.getLogger(__name__)


def is_error_response(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ–± –æ—à–∏–±–∫–µ"""
    if not content:
        return True
    return content.strip().startswith(("–û—à–∏–±–∫–∞", "–§–∞–π–ª", "Error"))


def keyword_search_in_files(query: str, top_n: int = 5, context_chars: int = 300) -> List[Dict]:
    """
    –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–∞—Ö (fallback).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
    """
    hits = []
    query_lower = query.lower()

    for filepath in BASE_FILES_DIR.iterdir():
        if not filepath.is_file():
            continue

        try:
            suffix = filepath.suffix.lower()

            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            if suffix in ['.xlsx', '.xls']:
                content = read_excel(filepath.name)
                is_table = True
            else:
                content = read_file(filepath)
                is_table = False

            if is_error_response(content):
                continue

            content_lower = content.lower()

            # –ò—â–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
            start = 0
            match_count = 0

            while True:
                pos = content_lower.find(query_lower, start)
                if pos == -1:
                    break

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ
                context_start = max(0, pos - context_chars)
                context_end = min(len(content), pos + len(query) + context_chars)
                snippet = content[context_start:context_end].replace("\n", " ").strip()

                # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞ –µ—Å–ª–∏ –æ–±—Ä–µ–∑–∞–Ω–æ
                prefix = "..." if context_start > 0 else ""
                suffix_text = "..." if context_end < len(content) else ""

                hits.append({
                    "filename": filepath.name,
                    "filetype": filepath.suffix.lstrip('.'),
                    "content": f"{prefix}{snippet}{suffix_text}",
                    "is_table": is_table,
                    "chunk_index": match_count,
                    "total_chunks": -1,  # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –¥–ª—è keyword –ø–æ–∏—Å–∫–∞
                    "score": 1.0,
                    "match_type": "keyword"
                })

                match_count += 1

                if len(hits) >= top_n:
                    return hits

                start = pos + 1

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ {filepath.name}: {e}")
            continue

    return hits


def semantic_search(query: str, user_id: str = "default", limit: int = 10) -> List[Dict]:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Weaviate"""
    if not vector_store.is_connected():
        logger.warning("Weaviate –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return []

    try:
        results = vector_store.search_documents(query, user_id, limit=limit)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞
        for r in results:
            r["match_type"] = "semantic"

        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []


def hybrid_search(query: str, user_id: str = "default", top_n: int = 5) -> List[Dict]:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: semantic + keyword.
    –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É, –∞ –Ω–µ –ø–æ filename (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —á–∞–Ω–∫–∞–º–∏).
    """

    # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    semantic_results = semantic_search(query, user_id, limit=top_n * 2)

    # 2. –ï—Å–ª–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –¥–∞–ª–∞ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º keyword
    if len(semantic_results) < 3:
        logger.info("–î–æ–ø–æ–ª–Ω—è–µ–º keyword –ø–æ–∏—Å–∫–æ–º...")
        keyword_results = keyword_search_in_files(query, top_n=top_n)

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤), –Ω–µ –ø–æ filename
        seen_content = {r["content"][:100] for r in semantic_results}

        for kr in keyword_results:
            content_key = kr["content"][:100]
            if content_key not in seen_content:
                semantic_results.append(kr)
                seen_content.add(content_key)

    # 3. –†–∞–Ω–∂–∏—Ä—É–µ–º: semantic –≤—ã—à–µ, –∑–∞—Ç–µ–º –ø–æ score
    semantic_results.sort(key=lambda x: (
        0 if x["match_type"] == "semantic" else 1,
        -x.get("score", 0)
    ))

    return semantic_results[:top_n]


def get_rag_context(query: str, user_id: str = "default", top_n: int = 5,
                    max_table_chars: int = 10000, max_doc_chars: int = 800) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è RAG/LLM –∞–≥–µ–Ω—Ç–∞.
    –¢–∞–±–ª–∏—Ü—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –¶–ï–õ–ò–ö–û–ú (–¥–æ max_table_chars).
    –û–±—ã—á–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—Ä–µ–∑–∞—é—Ç—Å—è –¥–æ max_doc_chars.
    """
    results = hybrid_search(query, user_id, top_n)

    if not results:
        return ""

    context_parts = []
    context_parts.append("=== –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í ===\n")

    for i, doc in enumerate(results, 1):
        doc_type = "–¢–ê–ë–õ–ò–¶–ê" if doc.get("is_table") else "–î–û–ö–£–ú–ï–ù–¢"
        chunk_info = ""

        if doc.get("total_chunks", 1) > 1:
            chunk_info = f" (—á–∞–Ω–∫ {doc.get('chunk_index', 0) + 1}/{doc.get('total_chunks', '?')})"

        # ‚úÖ –¢–∞–±–ª–∏—Ü—ã ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é, –¥–æ–∫—É–º–µ–Ω—Ç—ã ‚Äî –æ–±—Ä–µ–∑–∞–µ–º
        if doc.get("is_table"):
            content = doc["content"][:max_table_chars]
            if len(doc["content"]) > max_table_chars:
                content += "\n...[—Ç–∞–±–ª–∏—Ü–∞ –æ–±—Ä–µ–∑–∞–Ω–∞]"
        else:
            content = doc["content"][:max_doc_chars]
            if len(doc["content"]) > max_doc_chars:
                content += "..."

        context_parts.append(
            f"--- [{doc_type}] {doc['filename']}{chunk_info} ---\n"
            f"{content}\n"
        )

    return "\n".join(context_parts)


def search_documents(query: str, user_id: str = "default", top_n: int = 5) -> str:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ ‚Äî —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –î–ª—è RAG –∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π get_rag_context().
    """
    results = hybrid_search(query, user_id, top_n)

    if not results:
        return "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

    lines = ["üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:**\n"]

    for i, doc in enumerate(results, 1):
        content_preview = doc["content"][:400]
        if len(doc["content"]) > 400:
            content_preview += "..."

        # –ò–∫–æ–Ω–∫–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if doc.get("match_type") == "semantic":
            match_icon = "üéØ"
        else:
            match_icon = "üîç"

        doc_icon = "üìä" if doc.get("is_table") else "üìÑ"

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–∞—Ö
        chunk_info = ""
        if doc.get("total_chunks", 1) > 1:
            chunk_info = f" [—á–∞—Å—Ç—å {doc.get('chunk_index', 0) + 1}/{doc.get('total_chunks', '?')}]"

        lines.append(
            f"{doc_icon} **{i}. {doc['filename']}**{chunk_info} {match_icon}\n"
            f"{content_preview}\n"
        )

    return "\n".join(lines)


def perform_search(query: str, user_id: str = "default", top_n: int = 5) -> str:
    """Alias –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å router.py"""
    return search_documents(query, user_id, top_n)


def get_raw_results(query: str, user_id: str = "default", top_n: int = 5) -> List[Dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ (–¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è).
    """
    return hybrid_search(query, user_id, top_n)