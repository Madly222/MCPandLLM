# agent/agent.py
from agent.memory import memory
from agent.router import route_message
from agent.prompts import SYSTEM_PROMPT
from agent.models import send_to_llm
from vector_store import vector_store
from tools.search_tool import get_rag_context
import logging

logger = logging.getLogger(__name__)

# === –õ–ò–ú–ò–¢–´ ===
MAX_HISTORY_MESSAGES = 10
MAX_RAG_CONTEXT_CHARS = 5000


def _filter_user_assistant(messages: list) -> list:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Ä–æ–ª–µ–π 'user' –∏ 'assistant' –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ.
    """
    return [m for m in messages if m.get("role") in ("user", "assistant")]


async def agent_process(prompt: str, user_id: str):
    # –ò—Å—Ç–æ—Ä–∏—è ‚Äî —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (user/assistant)
    memory.clear_all_history()

    history = (memory.get_history(user_id) or [])[-MAX_HISTORY_MESSAGES:]

    # RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç
    rag_context = _build_rag_context(prompt, user_id)

    # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç (system + rag –µ—Å–ª–∏ –µ—Å—Ç—å)
    system_content = SYSTEM_PROMPT
    if rag_context:
        system_content += f"\n\n{rag_context}"

    # messages –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ route/send
    messages = [{"role": "system", "content": system_content}] + history
    messages.append({"role": "user", "content": prompt})

    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä
    total_chars = sum(len(m["content"]) for m in messages)
    logger.info(f"üìä –ü—Ä–æ–º–ø—Ç: {total_chars} —Å–∏–º–≤–æ–ª–æ–≤, ~{total_chars // 3} —Ç–æ–∫–µ–Ω–æ–≤")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    if vector_store.is_connected():
        vector_store.add_chat_message(prompt, "user", user_id)

    # route_message –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å (assistant_text, updated_messages) –∏–ª–∏ (None, messages)
    result, updated_messages = await route_message(messages, user_id)

    # –ï—Å–ª–∏ route_message –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª ‚Äî –ø–æ—Å—ã–ª–∞–µ–º –¥–∞–Ω–Ω—ã–µ LLM
    if result is None:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–µ–π—á–∞—Å —Ç–æ–ª—å–∫–æ messages (system + history + prompt)
        result = await send_to_llm(messages)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –≤ Vector DB
    if vector_store.is_connected():
        vector_store.add_chat_message(result, "assistant", user_id)

    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–°–¢–û–†–ò–ò: —Ç–æ–ª—å–∫–æ user/assistant, –∏ –º–∞–∫—Å–∏–º—É–º MAX_HISTORY_MESSAGES ---
    # –ë–µ—Ä—ë–º —Å—Ç–∞—Ä—É—é –∏—Å—Ç–æ—Ä–∏—é (history) + –Ω–æ–≤—ã–π –ø–∞—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π (user, assistant)
    new_entries = [{"role": "user", "content": prompt},
                   {"role": "assistant", "content": result}]

    combined = history + new_entries
    filtered = _filter_user_assistant(combined)
    memory.set_history(user_id, filtered[-MAX_HISTORY_MESSAGES:])

    return result


def _build_rag_context(query: str, user_id: str) -> str:
    """–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
    if not vector_store.is_connected():
        return ""

    try:
        parts = []

        # –î–æ–∫—É–º–µ–Ω—Ç—ã
        doc_context = get_rag_context(query, user_id, top_n=3)
        if doc_context:
            parts.append(doc_context)

        # –ü–∞–º—è—Ç—å (–∫–æ—Ä–æ—Ç–∫–æ)
        facts = vector_store.search_memory(query, user_id, limit=2)
        if facts:
            parts.append("=== –ü–ê–ú–Ø–¢–¨ ===\n" + "\n".join(f"‚Ä¢ {f}" for f in facts))

        result = "\n\n".join(parts)
        return result[:MAX_RAG_CONTEXT_CHARS]

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ RAG: {e}")
        return ""