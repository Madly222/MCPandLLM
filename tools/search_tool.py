# tools/search_tool.py
import logging
import re
from pathlib import Path
from typing import List, Dict, Optional

from vector_store import vector_store
from tools.utils import BASE_FILES_DIR
from tools.file_tool import read_file
from tools.excel_tool import read_excel

logger = logging.getLogger(__name__)


# ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ====================

def is_error_response(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ–± –æ—à–∏–±–∫–µ"""
    if not content:
        return True
    return content.strip().startswith(("–û—à–∏–±–∫–∞", "–§–∞–π–ª", "Error"))


def extract_filename_pattern(query: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ (–º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞).
    """
    # –ò—â–µ–º —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π 3+ —Å–∏–º–≤–æ–ª–æ–≤
    patterns = re.findall(r'\b[A-Za-z–ê-–Ø–∞-—è0-9_-]{3,}\b', query)

    # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop_words = {
        '–Ω–∞–π–¥–∏', '–ø–æ–∏—Å–∫', '–ø–æ–∫–∞–∂–∏', '–æ—Ç–∫—Ä–æ–π', '—Ñ–∞–π–ª', '—Ñ–∞–π–ª—ã', '–¥–æ–∫—É–º–µ–Ω—Ç',
        '—Ç–∞–±–ª–∏—Ü–∞', '—Ç–∞–±–ª–∏—Ü—ã', '–≤—Å–µ', '–≤—Å–µ—Ö', '–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
        'search', 'find', 'show', 'file', 'files', 'document', 'table'
    }

    patterns = [p for p in patterns if p.lower() not in stop_words]

    if patterns:
        return max(patterns, key=len)
    return ""


# ==================== –ú–ï–¢–û–î–´ –ü–û–ò–°–ö–ê ====================

def keyword_search_in_files(query: str, top_n: int = 5, context_chars: int = 300) -> List[Dict]:
    """
    –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–∞—Ö.
    –ü–æ–ª–µ–∑–µ–Ω –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: –Ω–æ–º–µ—Ä–∞, –∫–æ–¥—ã, –ò–ù–ù –∏ —Ç.–¥.
    """
    hits = []
    query_lower = query.lower()

    for filepath in BASE_FILES_DIR.iterdir():
        if not filepath.is_file():
            continue

        try:
            suffix = filepath.suffix.lower()

            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            if suffix in ['.xlsx', '.xls']:
                content = read_excel(filepath.name)
                is_table = True
            else:
                content = read_file(filepath)
                is_table = False

            if is_error_response(content):
                continue

            content_lower = content.lower()

            # –ò—â–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
            start = 0
            match_count = 0

            while True:
                pos = content_lower.find(query_lower, start)
                if pos == -1:
                    break

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ
                context_start = max(0, pos - context_chars)
                context_end = min(len(content), pos + len(query) + context_chars)
                snippet = content[context_start:context_end].replace("\n", " ").strip()

                # –ú–∞—Ä–∫–µ—Ä—ã –æ–±—Ä–µ–∑–∫–∏
                prefix = "..." if context_start > 0 else ""
                suffix_text = "..." if context_end < len(content) else ""

                hits.append({
                    "filename": filepath.name,
                    "filetype": filepath.suffix.lstrip('.'),
                    "content": f"{prefix}{snippet}{suffix_text}",
                    "is_table": is_table,
                    "chunk_index": match_count,
                    "total_chunks": -1,
                    "score": 1.0,
                    "match_type": "keyword"
                })

                match_count += 1

                if len(hits) >= top_n:
                    return hits

                start = pos + 1

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ keyword –ø–æ–∏—Å–∫–∞ –≤ {filepath.name}: {e}")
            continue

    return hits


def filename_search(query: str, user_id: str = "default", limit: int = 20) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    """
    pattern = extract_filename_pattern(query)
    if not pattern:
        return []

    if not vector_store.is_connected():
        return []

    try:
        results = vector_store.search_by_filename(pattern, user_id, limit=limit)
        for r in results:
            r["match_type"] = "filename"
        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∏–º–µ–Ω–∏: {e}")
        return []


def semantic_search(query: str, user_id: str = "default", limit: int = 10) -> List[Dict]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Weaviate (–ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É).
    """
    if not vector_store.is_connected():
        logger.warning("Weaviate –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return []

    try:
        results = vector_store.search_documents(query, user_id, limit=limit)
        for r in results:
            r["match_type"] = "semantic"
        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []


# ==================== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê ====================

def smart_search(query: str, user_id: str = "default", limit: int = 10) -> List[Dict]:
    """
    –£–º–Ω—ã–π –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫:
    1. –ü–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–±—ã—Å—Ç—Ä–æ, —Ç–æ—á–Ω–æ)
    2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–ø–æ —Å–º—ã—Å–ª—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
    3. Keyword fallback (—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)

    –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    """
    results = []
    seen = set()

    # –ï—Å–ª–∏ Weaviate –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî —Ç–æ–ª—å–∫–æ keyword
    if not vector_store.is_connected():
        logger.warning("Weaviate –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ keyword –ø–æ–∏—Å–∫")
        return keyword_search_in_files(query, top_n=limit)

    # –®–ê–ì 1: –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    pattern = extract_filename_pattern(query)
    if pattern:
        logger.info(f"üìÅ –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏: '{pattern}'")
        for doc in filename_search(query, user_id, limit=20):
            key = doc["filename"]
            if key not in seen:
                results.append(doc)
                seen.add(key)
        logger.info(f"   ‚Üí –ù–∞–π–¥–µ–Ω–æ –ø–æ –∏–º–µ–Ω–∏: {len(results)}")

    # –®–ê–ì 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    logger.info(f"üéØ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: '{query}'")
    semantic_results = semantic_search(query, user_id, limit=limit)
    added_semantic = 0
    for doc in semantic_results:
        key = doc["filename"]
        if key not in seen:
            results.append(doc)
            seen.add(key)
            added_semantic += 1
    logger.info(f"   ‚Üí –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π: {added_semantic}")

    # –®–ê–ì 3: Keyword fallback (–µ—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
    if len(results) < 3:
        logger.info(f"üîé Keyword fallback: '{query}'")
        keyword_results = keyword_search_in_files(query, top_n=limit)
        added_keyword = 0
        for doc in keyword_results:
            key = doc["filename"]
            if key not in seen:
                results.append(doc)
                seen.add(key)
                added_keyword += 1
        logger.info(f"   ‚Üí –î–æ–±–∞–≤–ª–µ–Ω–æ keyword: {added_keyword}")

    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: filename > semantic > keyword
    priority = {"filename": 0, "semantic": 1, "keyword": 2}
    results.sort(key=lambda x: (
        priority.get(x.get("match_type", "keyword"), 3),
        -x.get("score", 0)
    ))

    logger.info(f"üìä –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return results[:limit]


# ==================== –§–£–ù–ö–¶–ò–ò –î–õ–Ø RAG –ò –í–´–í–û–î–ê ====================

def get_rag_context(query: str, user_id: str = "default", top_n: int = 10,
                    max_table_chars: int = 8000, max_doc_chars: int = 800) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è RAG/LLM –∞–≥–µ–Ω—Ç–∞.
    –¢–∞–±–ª–∏—Ü—ã ‚Äî —Ü–µ–ª–∏–∫–æ–º (–¥–æ max_table_chars).
    –î–æ–∫—É–º–µ–Ω—Ç—ã ‚Äî –æ–±—Ä–µ–∑–∞—é—Ç—Å—è (–¥–æ max_doc_chars).
    """
    results = smart_search(query, user_id, limit=top_n)

    if not results:
        return ""

    context_parts = []
    context_parts.append("=== –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í ===\n")

    for i, doc in enumerate(results, 1):
        doc_type = "–¢–ê–ë–õ–ò–¶–ê" if doc.get("is_table") else "–î–û–ö–£–ú–ï–ù–¢"

        # –ò–∫–æ–Ω–∫–∞ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
        match_icons = {"filename": "üìÅ", "semantic": "üéØ", "keyword": "üîç"}
        match_icon = match_icons.get(doc.get("match_type", ""), "")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–∞—Ö
        chunk_info = ""
        if doc.get("total_chunks", 1) > 1:
            chunk_info = f" (—á–∞–Ω–∫ {doc.get('chunk_index', 0) + 1}/{doc.get('total_chunks', '?')})"

        # –ö–æ–Ω—Ç–µ–Ω—Ç: —Ç–∞–±–ª–∏—Ü—ã —Ü–µ–ª–∏–∫–æ–º, –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—Ä–µ–∑–∞–µ–º
        if doc.get("is_table"):
            content = doc["content"][:max_table_chars]
            if len(doc["content"]) > max_table_chars:
                content += "\n...[—Ç–∞–±–ª–∏—Ü–∞ –æ–±—Ä–µ–∑–∞–Ω–∞]"
        else:
            content = doc["content"][:max_doc_chars]
            if len(doc["content"]) > max_doc_chars:
                content += "..."

        context_parts.append(
            f"--- [{doc_type}] {doc['filename']}{chunk_info} {match_icon} ---\n"
            f"{content}\n"
        )

    return "\n".join(context_parts)


def search_documents(query: str, user_id: str = "default", top_n: int = 5) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    results = smart_search(query, user_id, limit=top_n)

    if not results:
        return "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

    lines = ["üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:**\n"]

    for i, doc in enumerate(results, 1):
        content_preview = doc["content"][:400]
        if len(doc["content"]) > 400:
            content_preview += "..."

        # –ò–∫–æ–Ω–∫–∏
        match_icons = {"filename": "üìÅ", "semantic": "üéØ", "keyword": "üîç"}
        match_icon = match_icons.get(doc.get("match_type", ""), "üîç")
        doc_icon = "üìä" if doc.get("is_table") else "üìÑ"

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–∞—Ö
        chunk_info = ""
        if doc.get("total_chunks", 1) > 1:
            chunk_info = f" [—á–∞—Å—Ç—å {doc.get('chunk_index', 0) + 1}/{doc.get('total_chunks', '?')}]"

        lines.append(
            f"{doc_icon} **{i}. {doc['filename']}**{chunk_info} {match_icon}\n"
            f"{content_preview}\n"
        )

    return "\n".join(lines)


# ==================== –ê–õ–ò–ê–°–´ –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò ====================

def perform_search(query: str, user_id: str = "default", top_n: int = 5) -> str:
    """Alias –¥–ª—è router.py"""
    return search_documents(query, user_id, top_n)


def get_raw_results(query: str, user_id: str = "default", top_n: int = 5) -> List[Dict]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)"""
    return smart_search(query, user_id, limit=top_n)


def hybrid_search(query: str, user_id: str = "default", top_n: int = 5) -> List[Dict]:
    """Alias –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return smart_search(query, user_id, limit=top_n)