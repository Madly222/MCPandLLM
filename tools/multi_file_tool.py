# tools/multi_file_tool.py
import logging
from typing import List, Dict, Optional
from pathlib import Path

from vector_store import vector_store
from tools.search_tool import hybrid_search
from agent.models import send_to_llm

logger = logging.getLogger(__name__)

# –õ–∏–º–∏—Ç—ã
MAX_CONTEXT_PER_FILE = 8000  # –°–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Ñ–∞–π–ª –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ
MAX_SUMMARY_LENGTH = 500  # –î–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
MAX_FILES_FOR_DIRECT = 3  # –î–æ 3 —Ñ–∞–π–ª–æ–≤ ‚Äî –ø–µ—Ä–µ–¥–∞—ë–º –Ω–∞–ø—Ä—è–º—É—é
MAX_TOTAL_CONTEXT = 30000  # –û–±—â–∏–π –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞


async def process_multiple_files(
        query: str,
        user_id: str,
        filenames: Optional[List[str]] = None,
        top_n: int = 10
) -> str:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

    - –î–æ 3 —Ñ–∞–π–ª–æ–≤: –ø–µ—Ä–µ–¥–∞—ë–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞–ø—Ä—è–º—É—é
    - 4+ —Ñ–∞–π–ª–æ–≤: Map-Reduce (—Å–∞–º–º–∞—Ä–∏ –∫–∞–∂–¥–æ–≥–æ ‚Üí –∏—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞)
    """

    # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª—ã
    if filenames:
        # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –∏–º–µ–Ω–∞–º
        docs = _get_files_by_names(filenames, user_id)
    else:
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        docs = hybrid_search(query, user_id, top_n=top_n)

    if not docs:
        return "‚ùå –§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ —Ñ–∞–π–ª–∞–º
    files_content = _group_by_filename(docs)
    num_files = len(files_content)

    logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {num_files} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    if num_files <= MAX_FILES_FOR_DIRECT:
        return await _direct_processing(query, files_content)
    else:
        return await _map_reduce_processing(query, files_content)


def _get_files_by_names(filenames: List[str], user_id: str) -> List[Dict]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∏–º–µ–Ω–∞–º —Ñ–∞–π–ª–æ–≤"""
    if not vector_store.is_connected():
        return []

    all_docs = []
    for filename in filenames:
        # –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        results = vector_store.search_documents(filename, user_id, limit=5)
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–æ—á–Ω–æ–º—É –∏–º–µ–Ω–∏
        for doc in results:
            if doc.get("filename", "").lower() == filename.lower():
                all_docs.append(doc)

    return all_docs


def _group_by_filename(docs: List[Dict]) -> Dict[str, Dict]:
    """
    –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –ø–æ —Ñ–∞–π–ª–∞–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {filename: {"content": str, "is_table": bool, "chunks": int}}
    """
    files = {}

    for doc in docs:
        filename = doc.get("filename", "unknown")

        if filename not in files:
            files[filename] = {
                "content": "",
                "is_table": doc.get("is_table", False),
                "filetype": doc.get("filetype", ""),
                "chunks": 0
            }

        files[filename]["content"] += doc.get("content", "") + "\n"
        files[filename]["chunks"] += 1

    return files


async def _direct_processing(query: str, files_content: Dict[str, Dict]) -> str:
    """
    –ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–¥–ª—è –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤).
    –ü–µ—Ä–µ–¥–∞—ë–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –≤ LLM.
    """
    context_parts = ["=== –î–ê–ù–ù–´–ï –ò–ó –§–ê–ô–õ–û–í ===\n"]

    total_chars = 0
    chars_per_file = MAX_TOTAL_CONTEXT // len(files_content)

    for filename, data in files_content.items():
        content = data["content"][:chars_per_file]
        if len(data["content"]) > chars_per_file:
            content += "\n...[–æ–±—Ä–µ–∑–∞–Ω–æ]"

        doc_type = "–¢–ê–ë–õ–ò–¶–ê" if data["is_table"] else "–î–û–ö–£–ú–ï–ù–¢"
        context_parts.append(f"--- [{doc_type}] {filename} ---\n{content}\n")
        total_chars += len(content)

    full_context = "\n".join(context_parts)

    messages = [
        {"role": "system", "content": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."},
        {"role": "user", "content": f"{full_context}\n\n**–ó–∞–¥–∞—á–∞:** {query}"}
    ]

    return await send_to_llm(messages)


async def _map_reduce_processing(query: str, files_content: Dict[str, Dict]) -> str:
    """
    Map-Reduce –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤).
    1. Map: –ø–æ–ª—É—á–∞–µ–º —Å–∞–º–º–∞—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    2. Reduce: –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–∞–º–º–∞—Ä–∏ –≤ –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç
    """

    # === MAP: –°–∞–º–º–∞—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ ===
    summaries = []

    for filename, data in files_content.items():
        content = data["content"][:MAX_CONTEXT_PER_FILE]
        doc_type = "—Ç–∞–±–ª–∏—Ü–∞" if data["is_table"] else "–¥–æ–∫—É–º–µ–Ω—Ç"

        map_messages = [
            {
                "role": "system",
                "content": f"–ö—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ ({doc_type}) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∑–∞–ø—Ä–æ—Å–∞. "
                           f"–ú–∞–∫—Å–∏–º—É–º {MAX_SUMMARY_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤. –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã, –±–µ–∑ –≤–æ–¥—ã."
            },
            {
                "role": "user",
                "content": f"**–§–∞–π–ª:** {filename}\n**–ó–∞–ø—Ä–æ—Å:** {query}\n\n**–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:**\n{content}"
            }
        ]

        try:
            summary = await send_to_llm(map_messages)
            summaries.append({
                "filename": filename,
                "summary": summary[:MAX_SUMMARY_LENGTH],
                "is_table": data["is_table"]
            })
            logger.info(f"‚úÖ –°–∞–º–º–∞—Ä–∏ –¥–ª—è {filename} –≥–æ—Ç–æ–≤–æ")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∞–º–º–∞—Ä–∏ {filename}: {e}")
            summaries.append({
                "filename": filename,
                "summary": f"[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}]",
                "is_table": data["is_table"]
            })

    # === REDUCE: –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞ ===
    summaries_text = "\n\n".join([
        f"üìÑ **{s['filename']}** ({'—Ç–∞–±–ª–∏—Ü–∞' if s['is_table'] else '–¥–æ–∫—É–º–µ–Ω—Ç'}):\n{s['summary']}"
        for s in summaries
    ])

    reduce_messages = [
        {
            "role": "system",
            "content": "–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–∞–º–º–∞—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
                       "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã."
        },
        {
            "role": "user",
            "content": f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n**–°–∞–º–º–∞—Ä–∏ —Ñ–∞–π–ª–æ–≤:**\n{summaries_text}"
        }
    ]

    final_response = await send_to_llm(reduce_messages)

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
    sources = ", ".join([s["filename"] for s in summaries])
    return f"{final_response}\n\n---\nüìÅ *–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}*"


async def summarize_all_user_files(user_id: str) -> str:
    """–°–≤–æ–¥–∫–∞ –ø–æ –í–°–ï–ú —Ñ–∞–π–ª–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    return await process_multiple_files(
        query="–î–∞–π –æ–±—â—É—é —Å–≤–æ–¥–∫—É –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º. –ß—Ç–æ –≤ –Ω–∏—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è?",
        user_id=user_id,
        top_n=50
    )


async def compare_files(filenames: List[str], user_id: str, aspect: str = "") -> str:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    query = f"–°—Ä–∞–≤–Ω–∏ —ç—Ç–∏ —Ñ–∞–π–ª—ã"
    if aspect:
        query += f" –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—é: {aspect}"

    return await process_multiple_files(
        query=query,
        user_id=user_id,
        filenames=filenames
    )