# tools/search_tool.py
import logging
import re
import json
from pathlib import Path
from typing import List, Dict, Optional, Any

from vector_store import vector_store
from tools.utils import BASE_FILES_DIR
from tools.file_tool import read_file
from tools.excel_tool import read_excel

logger = logging.getLogger(__name__)

# -----------------------
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã / –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# -----------------------
DEFAULT_KEYWORD_CONTEXT_CHARS = 300
MIN_PATTERN_LENGTH = 3
KEYWORD_STOP_WORDS = {
    '–Ω–∞–π–¥–∏', '–ø–æ–∏—Å–∫', '–ø–æ–∫–∞–∂–∏', '–æ—Ç–∫—Ä–æ–π', '—Ñ–∞–π–ª', '—Ñ–∞–π–ª—ã', '–¥–æ–∫—É–º–µ–Ω—Ç',
    '—Ç–∞–±–ª–∏—Ü–∞', '—Ç–∞–±–ª–∏—Ü—ã', '–≤—Å–µ', '–≤—Å–µ—Ö', '–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
    'search', 'find', 'show', 'file', 'files', 'document', 'table',
    '—Å–∫–æ–ª—å–∫–æ', '–ø—Ä–æ–µ–∫—Ç', '–ø—Ä–æ–µ–∫—Ç—ã', 'micb'  # micb –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å/–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
}


# -----------------------
# –•–µ–ª–ø–µ—Ä—ã
# -----------------------
def _to_text_from_table(raw: Any, max_chars: int = 20000) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç read_excel –∫ —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Ñ–æ—Ä–º–µ.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: str, list(dict), dict, pandas.DataFrame (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–ª–∏),
    –∏ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äî str(raw).
    –î–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç CSV-like preview (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ max_chars).
    """
    try:
        # –ï—Å–ª–∏ —É–∂–µ —Å—Ç—Ä–æ–∫–∞
        if isinstance(raw, str):
            return raw[:max_chars]

        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (—Ç–∏–ø–∏—á–Ω–∞—è export-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
        if isinstance(raw, list):
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤ CSV-like: –∑–∞–≥–æ–ª–æ–≤–∫–∏ + —Å—Ç—Ä–æ–∫–∏
            if len(raw) == 0:
                return ""
            first = raw[0]
            if isinstance(first, dict):
                headers = list(first.keys())
                lines = [", ".join(headers)]
                for row in raw:
                    vals = [str(row.get(h, "")) for h in headers]
                    lines.append(", ".join(vals))
                    # –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–ª–∏–Ω—ã
                    if sum(len(l) for l in lines) > max_chars:
                        lines.append("...[table truncated]")
                        break
                return "\n".join(lines)

        # –ï—Å–ª–∏ —ç—Ç–æ dict (–≤–æ–∑–º–æ–∂–Ω–æ mapping sheet->rows)
        if isinstance(raw, dict):
            # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å –∫–ª—é—á–∏ –∏ –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∞
            parts = []
            for k, v in raw.items():
                parts.append(f"=== Sheet: {k} ===")
                # v –º–æ–∂–µ—Ç –±—ã—Ç—å list of dicts
                if isinstance(v, list) and v:
                    headers = list(v[0].keys()) if isinstance(v[0], dict) else []
                    if headers:
                        parts.append(", ".join(headers))
                    for i, row in enumerate(v):
                        if isinstance(row, dict):
                            parts.append(", ".join(str(row.get(h, "")) for h in headers))
                        else:
                            parts.append(str(row))
                        if len("\n".join(parts)) > max_chars:
                            parts.append("...[sheet truncated]")
                            break
                else:
                    parts.append(str(v)[:max_chars])
                if len("\n".join(parts)) > max_chars:
                    break
            return "\n".join(parts)[:max_chars]

        # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å pandas DataFrame (–µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–µ—Ä–µ–¥–∞–ª)
        try:
            import pandas as pd
            if isinstance(raw, pd.DataFrame):
                # –ø–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ N —Å—Ç—Ä–æ–∫ –≤ CSV-—Ñ–æ—Ä–º–∞—Ç–µ
                csv = raw.head(100).to_csv(index=False)
                return csv[:max_chars]
        except Exception:
            pass

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Å—Ç—Ä–æ–∫–µ
        return str(raw)[:max_chars]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã –∫ —Ç–µ–∫—Å—Ç—É: {e}")
        return ""


def _normalize_content(raw: Any, is_table: bool = False) -> str:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –ª—é–±—ã–µ —Ç–∏–ø—ã –∫ —Å—Ç—Ä–æ–∫–µ.
    –î–ª—è —Ç–∞–±–ª–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–µ—Ç _to_text_from_table –¥–ª—è —á–∏—Ç–∞–µ–º–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.
    """
    if raw is None:
        return ""

    if is_table:
        return _to_text_from_table(raw)

    # –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ ‚Äî –æ–±—Ä–µ–∑–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
    if isinstance(raw, str):
        return raw

    # –ï—Å–ª–∏ —ç—Ç–æ bytes
    if isinstance(raw, (bytes, bytearray)):
        try:
            return raw.decode(errors="ignore")
        except Exception:
            return str(raw)

    # –ï—Å–ª–∏ —ç—Ç–æ list/dict/other ‚Äî –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É
    try:
        return str(raw)
    except Exception:
        try:
            return json.dumps(raw, ensure_ascii=False)
        except Exception:
            return repr(raw)


def is_error_response(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ–± –æ—à–∏–±–∫–µ"""
    if not content:
        return True
    return content.strip().startswith(("–û—à–∏–±–∫–∞", "–§–∞–π–ª", "Error"))


def extract_filename_pattern(query: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–µ–∑–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –∑–∞–ø—Ä–æ—Å–∞:
    - –Ω–∞—Ö–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π >= MIN_PATTERN_LENGTH
    - –∏—Å–∫–ª—é—á–∞–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Å–ª–æ–≤–æ —Å –Ω–∞–∏–±. –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç—å—é
    """
    if not query:
        return ""

    tokens = re.findall(r'\b[A-Za-z–ê-–Ø–∞-—è0-9_-]{%d,}\b' % MIN_PATTERN_LENGTH, query)
    tokens = [t for t in tokens if t.lower() not in KEYWORD_STOP_WORDS]

    if not tokens:
        return ""

    # –≤–µ—Ä–Ω—É—Ç—å —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ), –Ω–æ —É—á–µ—Å—Ç—å MICB-like tokens uppercase
    tokens.sort(key=lambda s: (-len(s), s))
    return tokens[0]


# -----------------------
# –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤–Ω—É—Ç—Ä–∏ —Ñ–∞–π–ª–æ–≤ (keyword)
# -----------------------
def keyword_search_in_files(query: str, top_n: int = 5, context_chars: int = DEFAULT_KEYWORD_CONTEXT_CHARS) -> List[Dict]:
    """
    –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–∞—Ö (—Ç–µ–∫—Å—Ç + —Ç–∞–±–ª–∏—Ü—ã, —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –∫ CSV-like preview).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å snippet'–∞–º–∏.
    """
    hits: List[Dict] = []
    query_lower = (query or "").lower().strip()
    if not query_lower:
        return hits

    for filepath in BASE_FILES_DIR.iterdir():
        if not filepath.is_file():
            continue

        try:
            suffix = filepath.suffix.lower()
            is_table = False

            # ----------------
            # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ (–±–µ–∑ –ø–∞–¥–µ–Ω–∏–π)
            # ----------------
            if suffix in {'.xlsx', '.xls', '.csv'}:
                raw = read_excel(filepath.name)
                is_table = True
                content = _normalize_content(raw, is_table=True)
            else:
                raw = read_file(filepath)
                content = _normalize_content(raw, is_table=False)

            if not isinstance(content, str):
                content = str(content)

            if is_error_response(content):
                continue

            content_lower = content.lower()

            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –≤—Ö–æ–∂–¥–µ–Ω–∏–π
            start = 0
            match_count = 0
            while True:
                pos = content_lower.find(query_lower, start)
                if pos == -1:
                    break

                context_start = max(0, pos - context_chars)
                context_end = min(len(content), pos + len(query) + context_chars)
                snippet = content[context_start:context_end].replace("\n", " ").strip()

                prefix = "..." if context_start > 0 else ""
                suffix_text = "..." if context_end < len(content) else ""

                hits.append({
                    "filename": filepath.name,
                    "filetype": suffix.lstrip('.'),
                    "content": f"{prefix}{snippet}{suffix_text}",
                    "is_table": is_table,
                    "chunk_index": match_count,
                    "total_chunks": -1,
                    "score": 1.0,
                    "match_type": "keyword"
                })

                match_count += 1
                if len(hits) >= top_n:
                    return hits

                start = pos + 1

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ keyword –ø–æ–∏—Å–∫–∞ –≤ {filepath.name}: {e}")
            continue

    return hits


# -----------------------
# –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç vector_store)
# -----------------------
def filename_search(query: str, user_id: str = "default", limit: int = 20) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ vector_store (metadata –ø–æ –∏–º–µ–Ω–∏).
    –ï—Å–ª–∏ vector_store –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [].
    """
    pattern = extract_filename_pattern(query)
    if not pattern:
        return []

    if not vector_store.is_connected():
        return []

    try:
        results = vector_store.search_by_filename(pattern, user_id, limit=limit) or []
        for r in results:
            r["match_type"] = "filename"
        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∏–º–µ–Ω–∏: {e}")
        return []


# -----------------------
# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (—á–µ—Ä–µ–∑ vector_store)
# -----------------------
def semantic_search(query: str, user_id: str = "default", limit: int = 10) -> List[Dict]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
    –û–∂–∏–¥–∞–µ—Ç, —á—Ç–æ vector_store.search_documents –≤–µ—Ä–Ω—ë—Ç —Å–ø–∏—Å–æ–∫ dict —Å –∫–ª—é—á–∞–º–∏ filename, content, score –∏ —Ç.–¥.
    """
    if not vector_store.is_connected():
        logger.warning("Weaviate –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return []

    try:
        results = vector_store.search_documents(query, user_id, limit=limit) or []
        for r in results:
            r["match_type"] = "semantic"
        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []


# -----------------------
# –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É–º–Ω—ã–π –ø–æ–∏—Å–∫
# -----------------------
def smart_search(query: str, user_id: str = "default", limit: int = 10) -> List[Dict]:
    """
    1) –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ (filename)
    2) –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    3) Keyword fallback (–ª–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ñ–∞–π–ª–∞–º)
    Dedup –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º `limit` —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    results: List[Dict] = []
    seen = set()

    # –ï—Å–ª–∏ weaviate –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω ‚Äî —Ç–æ–ª—å–∫–æ keyword –ø–æ–∏—Å–∫
    if not vector_store.is_connected():
        logger.warning("Weaviate –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ keyword –ø–æ–∏—Å–∫")
        return keyword_search_in_files(query, top_n=limit)

    # 1) –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏
    pattern = extract_filename_pattern(query)
    if pattern:
        logger.info(f"üìÅ –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏: '{pattern}'")
        name_hits = filename_search(query, user_id, limit=limit * 2)
        for doc in name_hits:
            key = doc.get("filename")
            if key and key not in seen:
                results.append(doc)
                seen.add(key)
        logger.info(f"   ‚Üí –ù–∞–π–¥–µ–Ω–æ –ø–æ –∏–º–µ–Ω–∏: {len(name_hits)}")

    # 2) –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    logger.info(f"üéØ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: '{query}'")
    semantic_results = semantic_search(query, user_id, limit=limit)
    added_semantic = 0
    for doc in semantic_results:
        key = doc.get("filename")
        if key and key not in seen:
            results.append(doc)
            seen.add(key)
            added_semantic += 1
    logger.info(f"   ‚Üí –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π: {added_semantic}")

    # 3) Keyword fallback, –µ—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if len(results) < 3:
        logger.info(f"üîé Keyword fallback: '{query}'")
        keyword_results = keyword_search_in_files(query, top_n=limit)
        added_keyword = 0
        for doc in keyword_results:
            key = doc.get("filename")
            if key and key not in seen:
                results.append(doc)
                seen.add(key)
                added_keyword += 1
        logger.info(f"   ‚Üí –î–æ–±–∞–≤–ª–µ–Ω–æ keyword: {added_keyword}")

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É: filename > semantic > keyword –∏ –ø–æ score
    priority = {"filename": 0, "semantic": 1, "keyword": 2}
    results.sort(key=lambda x: (priority.get(x.get("match_type", "keyword"), 3), -float(x.get("score", 0))))

    logger.info(f"üìä –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return results[:limit]

def get_rag_context(query: str, user_id: str = "default", top_n: int = 10,
                    max_table_chars: int = 8000, max_doc_chars: int = 800) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è RAG –∞–≥–µ–Ω—Ç–∞.
    –¢–∞–±–ª–∏—Ü—ã ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ CSV-preview (–¥–æ max_table_chars).
    –î–æ–∫—É–º–µ–Ω—Ç—ã ‚Äî –æ–±—Ä–µ–∑–∞—é—Ç—Å—è –¥–æ max_doc_chars.
    """
    results = smart_search(query, user_id, limit=top_n)
    if not results:
        return ""

    parts: List[str] = []
    parts.append("=== –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í ===\n")

    for i, doc in enumerate(results, 1):
        doc_type = "–¢–ê–ë–õ–ò–¶–ê" if doc.get("is_table") else "–î–û–ö–£–ú–ï–ù–¢"
        match_icons = {"filename": "üìÅ", "semantic": "üéØ", "keyword": "üîç"}
        match_icon = match_icons.get(doc.get("match_type", ""), "")

        chunk_info = ""
        total_chunks = doc.get("total_chunks", 1) or 1
        if total_chunks > 1:
            chunk_info = f" (—á–∞–Ω–∫ {doc.get('chunk_index', 0) + 1}/{total_chunks})"

        raw_content = doc.get("content", "")
        if doc.get("is_table"):
            content = raw_content[:max_table_chars]
            if len(raw_content) > max_table_chars:
                content += "\n...[—Ç–∞–±–ª–∏—Ü–∞ –æ–±—Ä–µ–∑–∞–Ω–∞]"
        else:
            content = raw_content[:max_doc_chars]
            if len(raw_content) > max_doc_chars:
                content += "..."

        parts.append(f"--- [{doc_type}] {doc['filename']}{chunk_info} {match_icon} ---\n{content}\n")

    return "\n".join(parts)

def search_documents(query: str, user_id: str = "default", top_n: int = 5) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ (–∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–≤—å—é).
    """
    results = smart_search(query, user_id, limit=top_n)
    if not results:
        return "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

    lines = ["üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:**\n"]
    for i, doc in enumerate(results, 1):
        content_preview = (doc.get("content") or "")[:400]
        if len(doc.get("content", "")) > 400:
            content_preview += "..."
        match_icons = {"filename": "üìÅ", "semantic": "üéØ", "keyword": "üîç"}
        match_icon = match_icons.get(doc.get("match_type", ""), "üîç")
        doc_icon = "üìä" if doc.get("is_table") else "üìÑ"

        chunk_info = ""
        total_chunks = doc.get("total_chunks", 1) or 1
        if total_chunks > 1:
            chunk_info = f" [—á–∞—Å—Ç—å {doc.get('chunk_index', 0) + 1}/{total_chunks}]"

        lines.append(f"{doc_icon} **{i}. {doc['filename']}**{chunk_info} {match_icon}\n{content_preview}\n")

    return "\n".join(lines)

def perform_search(query: str, user_id: str = "default", top_n: int = 5) -> str:
    return search_documents(query, user_id, top_n)


def get_raw_results(query: str, user_id: str = "default", top_n: int = 5) -> List[Dict]:
    return smart_search(query, user_id, limit=top_n)


def hybrid_search(query: str, user_id: str = "default", top_n: int = 5) -> List[Dict]:
    return smart_search(query, user_id, limit=top_n)