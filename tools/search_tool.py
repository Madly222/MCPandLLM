import logging
from pathlib import Path
from typing import List, Dict, Optional

from vector_store import vector_store  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π
from tools.utils import BASE_FILES_DIR
from tools.file_tool import read_file
from tools.excel_tool import read_excel

logger = logging.getLogger(__name__)


def keyword_search_in_files(query: str, top_n: int = 5) -> List[Dict]:
    """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–∞—Ö (fallback)"""
    hits = []
    query_lower = query.lower()

    for filepath in BASE_FILES_DIR.iterdir():
        if not filepath.is_file():
            continue

        try:
            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            if filepath.suffix.lower() in ['.xlsx', '.xls']:
                content = read_excel(filepath.name)
            else:
                content = read_file(filepath)

            if not content or str(content).startswith(("–û—à–∏–±–∫–∞", "–§–∞–π–ª")):
                continue

            content_lower = content.lower()

            # –ò—â–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
            start = 0
            while True:
                pos = content_lower.find(query_lower, start)
                if pos == -1:
                    break

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ
                context_start = max(0, pos - 150)
                context_end = min(len(content), pos + len(query) + 150)
                snippet = content[context_start:context_end].replace("\n", " ").strip()

                hits.append({
                    "filename": filepath.name,
                    "filetype": filepath.suffix.lstrip('.'),
                    "content": snippet,
                    "score": 1.0,
                    "match_type": "keyword"
                })

                if len(hits) >= top_n:
                    return hits

                start = pos + 1

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ {filepath.name}: {e}")
            continue

    return hits


def semantic_search(query: str, user_id: str = "default", limit: int = 10) -> List[Dict]:

    """caca"""
    user_id = "default"
    """caca"""

    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Weaviate"""
    if not vector_store.is_connected():
        return []

    try:
        results = vector_store.search_documents(query, user_id, limit=limit)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞
        for r in results:
            r["match_type"] = "semantic"

        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []


def hybrid_search(query: str, user_id: str = "default", top_n: int = 5) -> List[Dict]:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: semantic + keyword"""

    """caca"""
    user_id = "default"
    """caca"""

    # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (—Ç–æ–ø-10)
    semantic_results = semantic_search(query, user_id, limit=10)

    # 2. –ï—Å–ª–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –¥–∞–ª–∞ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º keyword
    if len(semantic_results) < 3:
        logger.info("–î–æ–ø–æ–ª–Ω—è–µ–º keyword –ø–æ–∏—Å–∫–æ–º...")
        keyword_results = keyword_search_in_files(query, top_n=5)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        seen_files = {r["filename"] for r in semantic_results}
        for kr in keyword_results:
            if kr["filename"] not in seen_files:
                semantic_results.append(kr)
                seen_files.add(kr["filename"])

    # 3. –†–∞–Ω–∂–∏—Ä—É–µ–º: semantic —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—à–µ
    semantic_results.sort(key=lambda x: 0 if x["match_type"] == "semantic" else 1)

    return semantic_results[:top_n]


def search_documents(query: str, user_id: str = "default", top_n: int = 5) -> str:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –¥–ª—è router"""

    """caca"""
    user_id = "default"
    """caca"""

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
    results = hybrid_search(query, user_id, top_n)

    if not results:
        return "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    lines = ["üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:**\n"]

    for i, doc in enumerate(results, 1):
        content_preview = doc["content"][:300]
        if len(doc["content"]) > 300:
            content_preview += "..."

        match_type = "üéØ —Å–µ–º–∞–Ω—Ç–∏–∫–∞" if doc.get("match_type") == "semantic" else "üîç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"

        lines.append(
            f"üìÑ **{i}. {doc['filename']}** ({doc.get('filetype', '?')}) {match_type}\n"
            f"{content_preview}\n"
        )

    return "\n".join(lines)

def perform_search(query: str, user_id: str = "default", top_n: int = 5):
    """
    –°—Ç–∞—Ä—ã–π alias –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å router.py.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫.
    """

    """caca"""
    user_id = "default"
    """caca"""
    
    return search_documents(query, user_id, top_n)
